---
layout: post
title: 强化学习基础
categories: Research Notes
tags: [Reinforce_Learning LAMDA]

---

## 概念

强化学习是用来实现**序贯决策**的机器学习方法。

决策和预测任务不同，决策往往会带来“后果”，因此决策者需要为未来负责，在未来的时间点做出进一步的决策。

### 智能体

代表做决策的机器。

在每一轮交互中，智能体感知到环境目前所处的状态，经过自身的计算给出本轮的动作，将其作用到环境中；环境得到智能体的动作后，产生相应的即时奖励信号并发生相应的状态转移。智能体则在下一轮交互中感知到新的环境状态，依次类推。

**三要素：感知、决策和奖励**

- 感知：智能体在某种程度上感知环境的状态，从而知道自己所处的现状。例如，下围棋的智能体感知当前的棋盘情况。
- 决策：智能体根据当前的状态计算出达到目标需要采取的动作的过程叫作决策。例如，针对当前的棋盘决定下一颗落子的位置。
- 奖励：环境根据状态和智能体采取的动作，产生一个标量信号作为奖励反馈。这个标量信号衡量智能体这一轮动作的好坏。例如，围棋博弈是否胜利；无人车是否安全、平稳且快速地行驶。

因为决策任务是多轮的，智能体就需要在每轮做决策时考虑未来环境相应的改变，**所以当前轮带来最大奖励反馈的动作，在长期来看并不一定是最优的。**

### 环境

环境是动态变化的，即”演变“，是一个随机过程。

对于一个随机过程，其最关键的要素就是状态以及状态转移的条件概率分布。这就好比一个微粒在水中的布朗运动可以由它的起始位置以及下一刻的位置相对当前位置的条件概率分布来刻画。

将智能体的动作因素考虑进去，**那么环境的下一刻状态的概率分布将由当前状态和智能体的动作来共同决定：**
$$
P_{next} \backsim P(·|P_{curr},Action_{Agent} )
$$

### 目标

由于环境的演变是随机过程，所以我们关注回报的期望，定义为**价值**，这就是强化学习中智能体学习的优化目标。

### 强化学习的数据

有监督学习和强化学习在数据层面有很大区别：

* 有监督学习的任务建立在从给定的数据分布中采样得到的训练数据集上，通过优化在训练数据集中设定的目标函数（如最小化预测误差）来找到模型的最优参数。这里，训练数据集背后的数据分布是完全不变的。
* 在强化学习中，数据是在智能体与环境交互的过程中得到的。如果智能体不采取某个决策动作，那么该动作对应的数据就永远无法被观测到，所以**当前智能体的训练数据来自之前智能体的决策结果。**因此，智能体的策略不同，与环境交互所产生的数据分布就不同。

强化学习中有一个关于数据分布的概念，叫作占用度量（occupancy measure）。

归一化的占用度量用于衡量在一个智能体决策与一个动态环境的交互过程中，采样到一个具体的**状态动作对（state-action pair）**的概率分布。

占用度量有一个很重要的性质：给定两个策略及其与一个动态环境交互得到的两个占用度量，那么当且仅当这两个占用度量相同时，这两个策略相同。也就是说，如果一个智能体的策略有所改变，那么它和环境交互得到的占用度量也会相应改变。

根据占用度量这一重要的性质，我们可以领悟到强化学习本质的思维方式。

- 强化学习的策略在训练中会不断更新，其对应的数据分布（即占用度量）也会相应地改变。因此，强化学习的一大难点就在于，智能体看到的数据分布是随着智能体的学习而不断发生改变的。
- 由于奖励建立在状态动作对之上，一个策略对应的价值其实就是一个占用度量下对应的奖励的期望，因此寻找最优策略对应着寻找最优占用度量。

### 小结

强化学习任务的最终优化目标是最大化智能体策略在和动态环境**交互过程中的价值**。

策略的价值可以等价转换成奖励函数在策略的占用度量上的期望：
$$
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{equation}
  最优策略 = \argmax_{策略} \ \mathbb{E}(状态，动作) \backsim 
策略的占用度量[奖励函数(状态，动作)]
\end{equation}
$$
有监督学习直接通过优化模型对于数据特征的输出来优化目标，即修改目标函数而数据分布不变；强化学习则通过改变策略来调整智能体和环境交互数据的分布，进而优化目标，**即修改数据分布而目标函数不变。**

所以：

**强化学习关注寻找一个智能体策略，使其在与动态环境交互的过程中产生最优的数据分布，即最大化该分布下一个给定奖励函数的期望。**







