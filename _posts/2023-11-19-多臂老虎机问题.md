---
layout: post
title: 多臂老虎机问题
categories: Learning Notes
tags: [Reinforce_Learning LAMDA]
---

## 关键词

* 试错型学习
* 探索与利用

## 问题定义

在多臂老虎机（multi-armed bandit，MAB）问题（见图 2-1）中，有一个拥有K根拉杆的老虎机，拉动每一根拉杆都对应一个关于奖励的概率分布R。

我们每次拉动其中一根拉杆，就可以从该拉杆对应的奖励概率分布中获得一个奖励 。我们在各根拉杆的奖励概率分布未知的情况下，从头开始尝试，目标是在操作T次拉杆后获得尽可能高的累积奖励。

由于奖励的概率分布是未知的，因此我们需要在“探索拉杆的获奖概率”和“根据经验选择获奖最多的拉杆”中进行权衡。“采用怎样的操作策略才能使获得的累积奖励最高”便是多臂老虎机问题。

![img](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/640.8908144b.png)

## 要素形式化描述

多臂老虎机问题可以表示为一个元组$\langle \mathcal{A},\mathcal{R} \rangle$，其中

* $\mathcal{A}$是动作集合，包含K个元素（总共有K个老虎机）
* $\mathcal{R}$是奖励概率分布，也就是说每个老虎机的产出对应一个奖励概率分布（而不是固定的）。

多臂老虎机的目标是最大化一段时间步T内累计的奖励。
$$
max \sum^{T}_{t=1}r_{t},r_{t} \backsim \mathcal{R}(·|a_{t})
$$

## 累计懊悔

对于某个动作a，存在一个期望奖励，即其概率分布均值。那么存在一个期望最大的拉杆，表示为$Q^{*}$。

为了更加直观、方便地观察拉动一根拉杆的期望奖励离最优拉杆期望奖励的差距，我们引入**懊悔**（regret）概念。懊悔定义为拉动当前拉杆的动作a与最优拉杆的期望奖励差，即$R(a)=Q^{*}-Q(a)$。累计懊悔为$\sigma_{R}=\sum^{T}_{t=1}R(a_{t})$，即一次完整的T步决策完成后累计的懊悔总量。

MAB问题的目标为最大化累积奖励，等价于最小化累积懊悔。

## 估计期望奖励

由于只拉动一次拉杆获得的奖励存在随机性，所以需要多次拉动一根拉杆，然后计算得到的多次奖励的期望。（**增量式更新**）

## 平衡探索与利用：算法实现

**基本算法框架**

```Python
class Solver:
    """ 多臂老虎机算法基本框架 """
    def __init__(self, bandit):
        self.bandit = bandit
        self.counts = np.zeros(self.bandit.K)  # 每根拉杆的尝试次数
        self.regret = 0.  # 当前步的累积懊悔
        self.actions = []  # 维护一个列表,记录每一步的动作
        self.regrets = []  # 维护一个列表,记录每一步的累积懊悔

    def update_regret(self, k):
        # 计算累积懊悔并保存,k为本次动作选择的拉杆的编号
        self.regret += self.bandit.best_prob - self.bandit.probs[k]
        self.regrets.append(self.regret)

    def run_one_step(self):
        # 返回当前动作选择哪一根拉杆,由每个具体的策略实现
        raise NotImplementedError

    def run(self, num_steps):
        # 运行一定次数,num_steps为总运行次数
        for _ in range(num_steps):
            k = self.run_one_step()
            self.counts[k] += 1
            self.actions.append(k)
            self.update_regret(k)
```

在多臂老虎机问题中，一个经典的问题就是探索与利用的平衡问题。

**探索**（exploration）是指尝试拉动更多可能的拉杆，这根拉杆不一定会获得最大的奖励，但这种方案能够摸清楚所有拉杆的获奖情况。例如，对于一个10臂老虎机，我们要把所有的拉杆都拉动一下才知道哪根拉杆可能获得最大的奖励。

**利用**（exploitation）是指拉动已知期望奖励最大的那根拉杆，由于已知的信息仅仅来自有限次的交互观测，所以当前的最优拉杆不一定是全局最优的。

于是在多臂老虎机问题中，设计策略时就需要平衡探索和利用的次数，使得累积奖励最大化。一个比较常用的思路是在开始时做比较多的探索，在对每根拉杆都有比较准确的估计后，再进行利用。目前已有一些比较经典的算法来解决这个问题，例如上置信界算法和汤普森采样算法等，接下来将分别介绍这几种算法。

### $\epsilon$-贪心算法

在原来的完全贪婪算法上添加噪声，每次以$1-\epsilon$概率选择目前为止奖励最大的那根拉杆（**利用**），$\epsilon$概率随机选择一根拉杆（**探索**）。

随着探索次数的不断增加，我们对各个动作的奖励估计得越来越准，此时我们就没必要继续花大力气进行探索。所以在$\epsilon$-贪婪算法的具体实现中，我们可以令$\epsilon$随时间衰减，即探索的概率将会不断降低。

但是$\epsilon$不会在有限时间内衰减至0。因为基于**有限步数**观测的完全贪婪算法仍然是一个局部信息的贪婪算法，永远距离最优解有一个固定的差距。

固定$\epsilon$的懊悔：

![img](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/output_2_4_2.4825c3f7.png)

$\epsilon$与时间呈反比的懊悔：

![img](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/output_2_4_3.dd308d2c.png)

后者明显更优。

### 上置信界算法

* 第一根拉杆只被拉动过一次，得到的奖励为0。
* 第二根拉杆被拉动过很多次，我们对它的奖励分布已经有了大致的把握。

问题是要不要继续尝试第一根。

我们可以使用一种基于不确定性的策略来综合考虑现有的期望奖励估值和不确定性，其核心问题是如何估计不确定性。

**上置信界**（upper confidence bound，UCB）算法是一种经典的基于不确定性的策略算法，它的思想用到了一个非常著名的数学原理：**霍夫丁不等式**（Hoeffding's inequality）

在概率论中，霍夫丁不等式给出了随机变量的和与其期望值偏差的概率上限，该不等式被Wassily Hoeffding于1963年提出并证明。霍夫丁不等式是Azuma-Hoeffding不等式的特例。

令$X_1，\dots，X_n$为独立的随机变量，且$X_i\in[a,b],i=1，\dots，n$。这些随机变量的经验均值可表示为：$\bar{X}=\frac{X_1+\dots+X_n}{n}\\$。

霍夫丁不等式叙述如下：
$$
\forall{t>0}，\quad P(\bar{X}-E[\bar{X}]\ge t)\le exp(-\frac{2n^2t^2}{\begin{matrix} \sum_{i=1}^n (b_i-a_i)^2 \end{matrix}})\\
$$
对于伯努利分布来说随机变量的上下界是1和0，所以：
$$
\forall{t>0}，\quad P(\bar{X}-E[\bar{X}]\ge t)\le e^{-2nt^{2}}\\
$$
其中$t=\hat{U_{t}}(a)$代表不确定性度量。给定一个概率$p=e^{-2N_{t}(a)U_{t}(a)^{2}}$，根据不等式（反向）可以得到$Q_{t}(a)<\hat{Q_{t}}(a)+\hat{U_{t}}(a)$至少以概率$1-p$成立。 当p很小的时候$Q_{t}(a)<\hat{Q_{t}}(a)+\hat{U_{t}}(a)$就以很大概率成立。所以$\hat{Q_{t}}(a)+\hat{U_{t}}(a)$便是**期望奖励的上界**。

这里给定p之后，就可以解得$\hat{U_{t}}(a)=\sqrt{\frac{-log \ p}{2N_{t}(a)}}$。

总结：事先设定一个概率p。UCB算法在每次选择拉杆前，先估计每根拉杆的期望奖励的上界，使得拉动每根拉杆的期望奖励只有一个较小的概率p超过这个上界，接着选出期望奖励上界最大的拉杆，从而选择最有可能获得最大期望奖励的拉杆。

实现：设置$p=\frac{1}{t}$，为拉动每根拉杆的次数+1（防止分母为0），并设定一个系数c来控制不确定性的比重。

![img](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/output_2_5.53e4e9e1.png)

（似乎一般，不过没关系）

### 汤普森采样算法

假设拉动每根拉杆的奖励服从一个特定的概率分布，然后根据拉动每根拉杆的期望奖励来进行选择。

但是由于计算所有拉杆的期望奖励的代价比较高，汤普森采样算法使用采样的方式，即根据当前每个动作a的奖励概率分布进行一轮采样，得到一组各根拉杆的奖励样本，再选择样本中奖励最大的动作。可以看出，汤普森采样是一种计算所有拉杆的最高奖励概率的蒙特卡洛采样方法。

在实际情况中，我们通常用 Beta 分布对当前每个动作的奖励概率分布进行建模。具体来说，若某拉杆被选择k次，其中$m_{1}$次奖励为1，$m_{2}$次奖励为0，则该拉杆的奖励服从参数为$(m_{1}+1,m_{2}+1)$的 Beta分布。

![img](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/641.c16a3dee.png)

![img](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/output_2_6.45dec2db.png)

## 总结

 $\epsilon$-贪婪算法的累积懊悔是随时间线性增长的，而另外 3 种算法（ $\epsilon$-衰减贪婪算法、上置信界算法、汤普森采样算法）的累积懊悔都是随时间次线性增长的（具体为对数形式增长）。

多臂老虎机的每次交互的结果和以往的动作无关，所以可看作**无状态的强化学习**（stateless reinforcement learning）。