---
layout: post
title: 悬崖漫步环境——动态规划算法
categories: Learning Notes
tags: [Reinforce_Learning LAMDA]
---

动态规划的基本思想是将待求解问题分解成若干个子问题，先求解子问题，然后从这些子问题的解得到目标问题的解。

经典问题，例如背包问题和最短路径规划。

基于动态规划的强化学习算法主要有两种：

* **策略迭代**（policy iteration），由两部分组成

  * **策略评估**（policy evaluation）
  * **策略提升**（policy improvement）

  具体来说，策略迭代中的策略评估使用贝尔曼期望方程来得到一个策略的状态价值函数，这是一个动态规划的过程。

* **价值迭代**（value iteration），直接使用贝尔曼最优方程来进行动态规划，得到最终的最优状态价值。

悬崖漫步是一个非常经典的强化学习环境，它要求一个智能体从起点出发，避开悬崖行走，最终到达目标位置。如图所示，有一个 4×12 的网格世界，每一个网格表示一个状态。智能体的起点是左下角的状态，目标是右下角的状态，智能体在每一个状态都可以采取 4 种动作：上、下、左、右。如果智能体采取动作后触碰到边界墙壁则状态不发生改变，否则就会相应到达下一个状态。环境中有一段悬崖，智能体掉入悬崖或到达目标状态都会结束动作并回到起点，也就是说掉入悬崖或者达到目标状态是终止状态。智能体每走一步的奖励是 −1，掉入悬崖的奖励是 −100。

![img](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/540.f28e3c6f.png)

## 策略迭代算法

### 策略评估

策略迭代是策略评估和策略提升不断循环交替，直至最后得到最优策略的过程。

由贝尔曼期望方程，当知道奖励函数和状态转移函数时，我们可以根据下一个状态的价值来计算当前状态的价值。根据动态规划的思想，可以**把计算下一个可能状态的价值当成一个子问题，把计算当前状态的价值看作当前问题**。在得知子问题的解后，就可以求解当前问题。

更一般的，考虑所有的状态，就变成了用上一轮的状态价值函数来计算当前这一轮的状态价值函数，即
$$
V^{k+1}(s)=\sum_{a \in A}\pi(a|s)(r(s,a)+\gamma \sum_{s^{'} \in S}p(s^{'}|s,a)V^{k}(s^{'}))
$$
可以证明序列$\{V_{k}\}$会收敛到$V^{\pi}$，所以可以据此来计算得到一个策略的状态价值函数。但是这是无穷步（理想状态下），所以一般在$\max_{s \in \mathcal{S}}|V^{k+1}(s)-V^{k}(s)|$的值非常小，可以提前结束策略评估，提升效率

### 策略提升

通过策略评估已经知道了策略$\pi$的状态价值函数$V^{\pi}$之后，可以改进策略。

从某个状态的视角来看：

如果智能体在状态s下采取动作a，之后的动作依旧遵循策略$\pi$，此时得到的期望回报其实就是动作价值$Q^{\pi}(s,a)$（**理解！！**）。如果有$Q^{\pi}(s,a) > V^{\pi}(s)$，说明状态s下采取动作a会比原来的策略$\pi(a|s)$得到更高的期望回报。

从全部状态来看：

如果存在一个确定性策略$\pi'$，在任何一个状态s下都有$Q^{\pi}(s,a) \geq V^{\pi}(s)$，那么在任何状态下都有$V^{\pi'}(s) \geq V^{\pi}(s)$。

这便是**策略提升定理**（policy improvement theorem）。于是我们可以直接**贪心地在每一个状态选择动作价值最大的动作**，也就是
$$
\pi'(s)=\argmax_{a}Q^{\pi}(s,a)
$$
**当策略提升之后得到的策略和之前的策略一样时，说明策略迭代达到了收敛，此时和就是最优策略。**

### 策略更新的效果是怎样的？

在某些情况下，更新状态值函数可能导致在特定状态下以确定性方式选择某个动作。这并不是说一定会发生，而是取决于学习过程中状态值函数的估计情况。

考虑以下情况：

* **极端情况下：** 如果在某个状态下有一个动作的估计值远高于其他动作，而其他动作的值相对较低，那么通过策略提升，算法可能会以非常高的概率选择具有高估计值的动作，使得在这个状态下几乎总是选择这个动作。

* **较为平衡的情况：** 如果在某个状态下有几个动作的估计值相对平衡，更新状态值函数后，策略提升可能会导致在这个状态下选择其中一个动作，但并不是以确定性的方式。

### 策略迭代算法

![image-20231118191429066](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231118191429066.png)

结合策略评估和策略提升，我们得到以下策略迭代算法：

![image-20231118191603445](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231118191603445.png)

### 总结

* 必须先通过蒙特卡罗方法近似出状态价值函数
* 然后才能计算任何状态下任何动作的期望
* 然后更新策略，该策略下这个动作将会选择贝尔曼最优方程给出的动作
* 回到步骤一，在采样过程中我们不关心策略是怎样的

## 价值迭代算法

策略迭代中的策略评估需要进行很多轮才能收敛得到某一策略的状态函数，这需要很大的计算量，尤其是在状态和动作空间比较大的情况下。价值迭代是策略迭代的一种特殊情况。在某些情况下，我们可能不需要进行太多次的策略评估，而只进行一轮评估，然后直接根据这一轮的价值函数进行策略提升。这样的算法被称为价值迭代算法。

试想一下，可能出现这样的情况：虽然状态价值函数还没有收敛，但是不论接下来怎么更新状态价值，策略提升得到的都是同一个策略。

**"策略提升得到的都是同一个策略" 意味着尽管状态价值函数仍在更新和变化，但在某个时间点上，进行策略提升时得到的策略保持相对稳定，不会随着每一次状态价值函数的微小变化而发生显著的改变。**

具体来说，这可能是因为尽管状态价值函数还没有完全收敛，但在近期的更新中趋于稳定。这种情况下，即使状态价值函数的估计值在微小范围内变化，策略提升仍然会倾向于选择相同的动作，而不会发生剧烈的变化。这种稳定性可能是由于算法的设计、更新规则、学习率等因素的影响。

总的来说，尽管状态价值函数仍在变化，但在某个时间点上，策略提升得到的策略相对稳定，可能不会受到状态价值函数小幅变化的显著影响。这使得即使在状态价值函数还没有完全收敛的情况下，我们仍然可以在某种程度上依赖当前的策略来做决策。

确切来说，价值迭代可以看成一种动态规划过程，它利用的是贝尔曼最优方程**的迭代更新方式**：
$$
V^{k+1}(s)=\max_{a \in \mathcal{A}}\{r(s,a)+\gamma \sum_{s' \in \mathcal{S}}p(s'|s,a)V^{k}(s') \}
$$
等到状态价值函数不再变化时，它就是贝尔曼最优方程的不动点。

然后利用：
$$
\pi(s)=\argmax_{a}\{r(s,a)+\gamma \sum_{s'} p(s'|s,a)V^{k+1}(s')\}
$$
从中恢复出最优策略即可。

![image-20231118230802937](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231118230802937.png)

## 区别

价值迭代算法的结果和策略迭代算法的结果完全一致。