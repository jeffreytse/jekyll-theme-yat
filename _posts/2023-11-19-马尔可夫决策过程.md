---
layout: post
title: 马尔可夫决策过程
categories: Learning Notes
tags: [Reinforce_Learning LAMDA]
---

与多臂老虎机问题不同，**马尔可夫决策过程包含状态信息以及状态之间的转移机制。**

当且仅当某时刻的状态只取决于上一时刻的状态时，一个随机过程被称为具有**马尔可夫性质**（Markov property）。

我们用二元组$\langle \mathcal{S},\mathcal{P} \rangle$描述马尔可夫过程。前者是有限状态空间，后者是状态转移矩阵。

在马尔可夫过程的基础上加入奖励函数r和折扣因子γ，就可以得到**马尔可夫奖励过程**（Markov reward process），这是一个四元组，多出来的两个元素含义为：

* r是奖励函数，某个状态s的奖励$r(s)$指转移到该状态时可以获得奖励的期望。
* γ是折扣因子（discount factor），γ的取值范围为[0,1)。引入折扣因子的理由为远期利益具有一定不确定性，有时我们更希望能够尽快获得一些奖励，所以我们需要对远期利益打一些折扣。接近 1 的γ更关注长期的累计奖励，接近 0 的γ更考虑短期奖励。

## 回报

在一个马尔可夫奖励过程中，从第t时刻状态$S_{t}$开始（**实际上是给定了一个初始出发状态**），直到终止状态时，考虑衰减的所有奖励之和被称为回报$G_{t}$（Return）。
$$
G_{t}=\sum_{k=0}^{\infty}\gamma^{k}R_{t+k}
$$
这里是无限步。实际中取有限步。

## 价值函数

一个状态的期望回报（即从这个状态出发的未来累积奖励的期望）被称为这个状态的**价值**（value）。

所有状态的价值就组成了**价值函数**（value function）。这是一个有限集合的满射。
$$
V(s)=\mathbb{E}[G_{t}|S_{t}=s]=\mathbb{E}[R_{t}+\gamma V(S_{t+1})|S_{t}=s]
$$
拆解这个期望，前者是即时奖励的期望（就是该状态的奖励），后者是当前为s的条件下下一状态的奖励期望，即
$$
V(s)=r(s)+\gamma \sum_{s^{'} \in S}p(s^{'}|s)V(s^{'})
$$
此即**贝尔曼方程**（Bellman equation）。

扩展到所有状态：
$$
\mathcal{V}=\mathcal{R}+\gamma \mathcal{PV}
$$
可以直接根据矩阵运算求解（解析解）：
$$
\mathcal{V}=\mathcal{R}+\gamma \mathcal{PV} \\
(I-\gamma \mathcal{P})\mathcal{V}=\mathcal{R} \\
\mathcal{V}=(I-\gamma \mathcal{P})^{-1}\mathcal{R}
$$
计算复杂度$O(n^{3})$，适用于小规模马尔可夫奖励过程。

求解较大规模的马尔可夫奖励过程中的价值函数时，可以使用**动态规划**（dynamic programming）算法、**蒙特卡洛方法**（Monte-Carlo method）和**时序差分**（temporal difference）。

## 马尔可夫决策过程

在MRP上加入动作，就得到了马尔可夫决策过程（MDP）。MDP由五元组$\langle \mathcal{S},\mathcal{A},\mathcal{P},r,\gamma\rangle$构成。

注意：

* $r(s,a)$是奖励函数，同时取决于状态s和动作a
* $P(s^{'}|s,a)$是状态转移函数，表示在状态s执行动作a之后到达状态$s^{'}$的概率。所以$\mathcal{P}$变成三维的了。
* **MDP本身是不包含策略$\pi$的**

智能体与环境MDP的交互示意图：

![img](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/rl-process.723b4a67.png)

## 策略

智能体的策略用$\pi$表示，$\pi(a|s)=P(A_{t}=a|S_{t}=s)$是一个函数，表示在输入状态s情况下采取动作a的概率。

* **确定性策略**（deterministic policy）：在每个状态时只输出一个确定性的动作，即只有该动作的概率为 1，其他动作的概率为 0。
* **随机性策略**（stochastic policy）：在每个状态时输出的是关于动作的概率分布，然后根据该分布进行采样就可以得到一个动作。

## 状态价值函数

基于策略$\pi$的状态价值函数（state-value function），定义为从状态s出发遵循策略$\pi$能获得的期望回报，数学表达为：
$$
V^{\pi}(s)=\mathbb{E_{\pi}}[G_{t}|S_{t}=s]
$$

## 动作价值函数

在 MDP 中，由于动作的存在，我们额外定义一个**动作价值函数**（action-value function）。

$Q^{\pi}(s,a)$表示在MDP遵循策略$\pi$时，对当前状态s执行动作a得到的期望回报：
$$
Q^{\pi}(s,a)=\mathbb{E}_{\pi}[G_{t}|S_{t}=s,A_{t}=a]
$$
显然有：
$$
V^{\pi}(s)=\sum_{a \in A}\pi(a|s)Q^{\pi}(s,a)
$$
使用策略$\pi$时，状态s下采取动作a的价值等于即时奖励加上经过衰减后的所有可能的下一个状态的状态转移概率与相应的价值的乘积：
$$
Q^{\pi}(s,a)=r(s,a)+\gamma \sum_{s^{'} \in S}p(s^{'}|s,a)V^{\pi}(s^{'})
$$

**AI如何控制agent玩游戏？两条路：可以学习策略$\pi$或者学习动作价值函数$Q^{\pi}(s,a)$。**

## 贝尔曼期望方程

通过简单推导就可以分别得到两个价值函数的**贝尔曼期望方程**（Bellman Expectation Equation）:
$$
V^{\pi}(s)=\mathbb{E_{\pi}}[R_{t}+\gamma V^{\pi}(S_{t+1})|S_{t}=s] \\
=\sum_{a \in A}\pi(a|s)(r(s,a)+\gamma \sum_{s^{'} \in S}p(s^{'}|s,a)V^{\pi}(s^{'}))
$$

$$
Q^{\pi}(s,a)=\mathbb{E_{\pi}}[R_{t}+\gamma Q^{\pi}(S_{t+1},A_{t+1})|S_{t}=s,A_{t}=a] \\
=r(s,a)+\gamma \sum_{s^{'} \in S}p(s^{'}|s,a)\sum_{a^{'} \in A}\pi(a^{'}|s^{'}Q^{\pi}(s^{'},a^{'}))
$$

![img](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/mdp.aaacb46a.png)

**注意，这时候奖励就取决于在状态s下采取的动作a了**。

例如，为：

* 状态转移函数的一条表示："s1-保持s1-s1": 1.0（**形式为当前状态-采取的动作-下一步到达的状态：到达这个状态的概率**）
* 奖励函数的一条表示："s1-保持s1": -1（**当前状态s和动作a共同决定的Reward**）
* 策略的一条表示："s4-前往s5": 0.5（**当前状态-采取的动作：采取这个动作的概率**）

### 从MDP到MRP

我们注意到（到目前为止）MDP和MRP的定义方式是不同的，那么我们考虑给定一个MDP和一个策略$\pi$，是否可以将其转化为一个MRP？

答案是肯定的。我们可以将策略的动作选择进行**边缘化**（marginalization)，就可以得到没有动作的 MRP了。具体来说，对于某一个状态，我们根据策略所有动作的概率进行加权，得到的奖励和就可以认为是一个MRP在该状态下的奖励，即：
$$
r^{'}(s)=\sum_{a \in A}\pi(a|s)r(s,a)
$$
**也就是说，我们当前状态和执行动作共同决定的奖励通过概率加权的方式归到了状态本身。（积分积掉了）**

同理，在MDP中从s转移到下一个状态$s^{'}$的概率也可以通过概率加权的方式消除动作这个因素：
$$
P^{'}(s^{'}|s)=\sum_{a \in \mathcal{A}}\pi (a|s)P(s^{'}|s,a)
$$
根据价值函数的定义可以发现，转化前的 MDP 的状态价值函数和转化后的 MRP 的价值函数是一样的。于是我们可以用 MRP 中计算价值函数的解析解来计算这个 MDP 中该策略的状态价值函数。

## 蒙特卡洛方法

**蒙特卡洛方法**（Monte-Carlo methods）也被称为统计模拟方法，是一种基于概率统计的数值计算方法。

简单理解：用蒙特卡洛方法估计圆的面积

![用蒙特卡洛方法估计圆的面积](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/mc.c89f09b0.png)

如何用蒙特卡洛方法来估计一个策略在一个马尔可夫决策过程中的状态价值函数？

回忆一下，在某个策略$\pi$下，一个状态的价值是它的期望回报，那么一个很直观的想法就是用策略$\pi$在 MDP上采样很多条序列，计算从这个状态出发的回报再求其期望就可以了，公式如下：
$$
V^{\pi}(s)=\mathbb{E_{\pi}}[G_{t}|S_{t}=s] \thickapprox \frac{1}{N}\sum_{i=1}^{N}G_{t}^{(i)}
$$
这里介绍的蒙特卡洛价值估计方法会**在该状态每一次出现**时计算它的回报。

（还有一种选择是一条序列只计算一次回报，也就是这条序列第一次出现该状态时计算后面的累积奖励，而后面再次出现该状态时，该状态就被忽略了。）

我们为每一个状态维护一个计数器和总回报，计算状态价值的具体过程如下所示。

* 使用策略$\pi$采样若干条序列
* 对每一条序列中的每一时间步t的状态s进行以下操作：
  * 更新**状态s的计数器**$N(s) \leftarrow N(s)+1$
  * 更新**状态s的总回报**$M(s) \leftarrow M(s)+G_{t}$
  * 可以采用增量更新。
* 每一个状态的价值被估计为回报的平均值$V_{s}=\frac{M(s)}{N(s)}$

根据大数定律，当$N(s) \rightarrow \infty$，有$V(s) \rightarrow V^{\pi}(s)$。

这样可以估计得到**MDP的状态价值**，和MRP解析解得到的状态价值很接近。

## 占用度量

因为对于同一个MDP，不同策略会访问到的状态的概率分布是不同的，所以不同策略的价值函数是不一样的。

定义MDP的初始状态分布（即最初的分布概率）为$v_{0}(s)$，用$P_{t}^{\pi}(s)$来表示采取策略$\pi$使得智能体在t状态时刻为s的概率，所以由$P_{0}^{\pi}(s)=v_{0}(s)$，然后可以定义一个策略的状态访问分布：
$$
v^{\pi}(s)=(a-\gamma)\sum_{t=0}^{\infty}\gamma^{t}P_{t}^{\pi}(s)
$$
**状态访问概率表示一个策略和 MDP 交互会访问到的状态的分布。**（即经过无穷步后访问到各个状态的频率统计）

需要注意的是，理论上在计算该分布时需要交互到无穷步之后，但实际上智能体和 MDP 的交互在一个序列中是有限的。

不过我们仍然可以用以上公式来表达状态访问概率的思想，状态访问概率有如下性质：
$$
v^{\pi}(s')=(1-\gamma)v_{0}(s')+\gamma \int P(s'|s,a)\pi(a|s)v^{\pi}(s)dsda
$$
这个公式的直观理解是：一个状态的访问概率，等于初始的访问概率加上未来可能的所有状态的访问概率的期望，其中每个未来状态的访问概率被其转移概率和策略的概率加权。未来的访问概率是被折扣的，折扣因子γ表示了对未来访问的偏好程度，γ越接近1，表示越看重未来的访问。

**（这是什么？看不太懂）**

然后可以定义**策略的占用度量**：
$$
\rho^{\pi}(s,a)=(1-\gamma)\sum_{t=0}^{\infty}\gamma^{t}P_{t}^{\pi}(s)\pi(a|s)
$$
它表示动作状态对$(s,a)$被访问到的概率。

这个公式的含义是：在策略π下，状态-动作对(s,a)被访问的概率等于（1-γ）乘以从时间步0到无穷大的，状态s在时间步t被访问的概率乘以在状态s下选择动作a的概率，然后再乘以γ的t次方的和。这个公式考虑了所有可能的时间步，γ的t次方是对未来的折扣。

二者有关系：
$$
\rho^{\pi}(s,a)=v^{\pi}(s)\pi(a|s)
$$
由此可以推出两个重要定理：

* 由于$v^{\pi}(s)$与策略无关，只与状态转移有关，所以
  $$
  \rho^{\pi_{1}}=\rho^{\pi_{2}} \iff \pi_{1}=\pi_{2}
  $$

* 给定一个合法（可能存在的）占用度量$\rho$，可生成该占用度量的唯一策略是
  $$
  \pi_{\rho}=\frac{\rho (s,a)}{\sum_{a'}\rho(s,a')}
  $$

## 最优策略

强化学习的目标通常是找到一个策略，使得智能体从初始状态出发能获得最多的期望回报。

我们首先定义策略之间的偏序关系：当且仅当对于任意的状态s都有$V^{\pi}(s) \ge V^{\pi '}(s)$，有$\pi \ge \pi '$。所以**在Pareto意义上**至少有一个最优策略（可能有很多个），都表示为$\pi^{*}(s)$。然后可以定义：

* 最优状态价值函数，表示为
  $$
  V^{*}(s)=\max_{\pi}V^{\pi}(s)
  $$

* 最优动作价值函数，表示为
  $$
  Q^{*}(s,a)=\max_{\pi}Q^{\pi}(s,a) \ \ \  \forall s \in S,a \in \mathcal{A}
  $$

为了使$Q^{\pi}(s,a)$最大，我们需要在当前的状态动作对$(s,a)$之后都执行最优策略，于是我们得到了最优状态价值函数和最优动作价值函数之间的关系：
$$
Q^{*}(s,a)=r(s,a)+\gamma \sum_{s' \in S}P(s'|s,a)V^{*}(s')
$$
这与在普通策略下的状态价值函数和动作价值函数之间的关系是一样的。另一方面，最优状态价值是选择此时使最优动作价值最大的那一个动作时的状态价值：
$$
V^{*}(s)=\max_{a \in \mathcal{A}}Q^{*}(s,a)
$$

## 贝尔曼最优方程

根据$V^{*}(s)$和$Q^{*}(s,a)$的关系，我们可以得到**贝尔曼最优方程**（Bellman optimality equation）：
$$
V^{*}(s)=\max_{a \in \mathcal{A}}\{r(s,a)+\gamma \sum_{s' \in \mathcal{S}}p(s'|s,a)V^{*}(s') \} \\
Q^{*}(s,a)=r(s,a)+\gamma \sum_{s' \in \mathcal{S}}p(s'|s,a)\max_{a' \in \mathcal{A}}Q^{*}(s',a')
$$

## 小结

本章从零开始介绍了马尔可夫决策过程的基础概念知识，并讲解了**如何通过求解贝尔曼方程得到状态价值的解析解**以及**如何用蒙特卡洛方法估计各个状态的价值**。马尔可夫决策过程是强化学习中的基础概念，强化学习中的环境就是一个马尔可夫决策过程。我们接下来将要介绍的强化学习算法通常都是在求解马尔可夫决策过程中的最优策略。